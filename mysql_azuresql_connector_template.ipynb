{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import when\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import * \n",
    "from pyspark.sql.functions import lit\n",
    "import pyodbc\n",
    "import pandas as pd\n",
    "import pyspark.sql.functions as sf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using MySQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/17 15:37:53 WARN Utils: Your hostname, Marcus-MacBook.local resolves to a loopback address: 127.0.0.1; using 192.168.1.4 instead (on interface en0)\n",
      "24/02/17 15:37:53 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "24/02/17 15:37:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession for MySQL\n",
    "spark = SparkSession.builder.appName(\"test\").config('spark.driver.extraClassPath', '/opt/homebrew/Cellar/apache-spark/3.5.0/libexec/jars/mysql-connector-j-8.2.0.jar').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MySQL configurations\n",
    "database = 'YOUR_DATABASE_NAME'\n",
    "url = f'jdbc:mysql://localhost/{database}'\n",
    "driver = 'com.mysql.cj.jdbc.Driver' # New driver for MySQL\n",
    "dbtable = 'YOUR_TABLE_NAME'\n",
    "user = 'YOUR_USER_NAME' # Default user: root\n",
    "password = 'YOUR_PASSWORD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from MySQL\n",
    "df = spark.read.format('jdbc').option('url', url) \\\n",
    "    .option('driver', driver) \\\n",
    "    .option('dbtable', dbtable).option('user', user).option('password', password).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+-------+-----------+-----------+------------------+---------+---+----------+-----------+-------------+\n",
      "|departure|arrival|depart_time|arrive_time|           company|    price| id|date_leave|date_return|scraping_date|\n",
      "+---------+-------+-----------+-----------+------------------+---------+---+----------+-----------+-------------+\n",
      "|      HCM|    HAN|      21:00|      23:10|       VietJet Air|6,180,000|  1|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      21:00|      23:10|       VietJet Air|6,352,800|  2|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      05:00|      07:05|  Vietnam Airlines|7,050,000|  3|2024-02-06| 2024-02-18|   2023-12-25|\n",
      "|      HCM|    HAN|      12:10|      02:20|       VietJet Air|6,007,200|  4|2024-02-05| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      22:50|      01:00|       VietJet Air|6,180,000|  5|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      21:40|      23:50|       VietJet Air|6,352,800|  6|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      05:00|      07:20|  Vietnam Airlines|7,050,000|  7|2024-02-06| 2024-02-18|   2023-12-25|\n",
      "|      HCM|    HAN|      23:50|      01:55|       VietJet Air|6,180,000|  8|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      12:05|      02:15|Vietravel Airlines|6,262,080|  9|2024-02-05| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      23:50|      01:55|       VietJet Air|6,352,800| 10|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      06:00|      08:10|  Vietnam Airlines|7,050,000| 11|2024-02-06| 2024-02-18|   2023-12-25|\n",
      "|      HCM|    HAN|      08:05|      10:15|       VietJet Air|6,374,400| 12|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      04:30|      06:40|       VietJet Air|6,547,200| 13|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      12:20|      02:30|Vietravel Airlines|6,262,080| 14|2024-02-05| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      09:30|      11:40|       VietJet Air|6,374,400| 15|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      05:15|      07:25|       VietJet Air|6,547,200| 16|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      06:15|      08:25|  Vietnam Airlines|7,050,000| 17|2024-02-06| 2024-02-18|   2023-12-25|\n",
      "|      HCM|    HAN|      10:10|      00:20|       VietJet Air|6,374,400| 18|2024-02-06| 2024-02-16|   2023-12-25|\n",
      "|      HCM|    HAN|      06:00|      08:10|       VietJet Air|6,547,200| 19|2024-02-05| 2024-02-17|   2023-12-25|\n",
      "|      HCM|    HAN|      14:50|      16:55|  Vietnam Airlines|7,050,000| 20|2024-02-05| 2024-02-16|   2023-12-25|\n",
      "+---------+-------+-----------+-----------+------------------+---------+---+----------+-----------+-------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Write to MySQL\n",
    "df.write.format('jdbc').option('url',url).option('driver',driver).option('dbtable','customer_behaviour').option('user',user).option('password',password).mode('append').save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using AzureSQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "24/02/27 09:06:07 WARN Utils: Your hostname, Marcus-MacBook.local resolves to a loopback address: 127.0.0.1; using 192.168.1.6 instead (on interface en0)\n",
      "24/02/27 09:06:07 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ":: loading settings :: url = jar:file:/Users/marcusle02/opt/anaconda3/lib/python3.9/site-packages/pyspark/jars/ivy-2.5.1.jar!/org/apache/ivy/core/settings/ivysettings.xml\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Ivy Default Cache set to: /Users/marcusle02/.ivy2/cache\n",
      "The jars for the packages stored in: /Users/marcusle02/.ivy2/jars\n",
      "com.microsoft.azure#spark-mssql-connector added as a dependency\n",
      ":: resolving dependencies :: org.apache.spark#spark-submit-parent-64e05673-39a6-4c0a-8cb2-ff3e02075d2c;1.0\n",
      "\tconfs: [default]\n",
      "\tfound com.microsoft.azure#spark-mssql-connector;1.0.2 in central\n",
      "\tfound com.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 in central\n",
      ":: resolution report :: resolve 95ms :: artifacts dl 3ms\n",
      "\t:: modules in use:\n",
      "\tcom.microsoft.azure#spark-mssql-connector;1.0.2 from central in [default]\n",
      "\tcom.microsoft.sqlserver#mssql-jdbc;8.4.1.jre8 from central in [default]\n",
      "\t---------------------------------------------------------------------\n",
      "\t|                  |            modules            ||   artifacts   |\n",
      "\t|       conf       | number| search|dwnlded|evicted|| number|dwnlded|\n",
      "\t---------------------------------------------------------------------\n",
      "\t|      default     |   2   |   0   |   0   |   0   ||   2   |   0   |\n",
      "\t---------------------------------------------------------------------\n",
      ":: retrieving :: org.apache.spark#spark-submit-parent-64e05673-39a6-4c0a-8cb2-ff3e02075d2c\n",
      "\tconfs: [default]\n",
      "\t0 artifacts copied, 2 already retrieved (0kB/3ms)\n",
      "24/02/27 09:06:07 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n"
     ]
    }
   ],
   "source": [
    "# Create SparkSession for AzureSQL\n",
    "spark = SparkSession.builder.config('spark.jars.packages','com.microsoft.azure:spark-mssql-connector:1.0.2').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# AzureSQL configurations\n",
    "jdbcHostname = \"YOUR_HOST_NAME\"\n",
    "jdbcDatabase = \"YOUR_DATABASE\"\n",
    "jdbcPort = \"1433\"\n",
    "username = \"YOUR_USER\"\n",
    "password = \"YOUR_PASSWORD\"\n",
    "jdbcUrl = \"jdbc:sqlserver://{0}:{1};database={2}\".format(jdbcHostname, jdbcPort, jdbcDatabase)\n",
    "\n",
    "connectionProperties = {\n",
    "\"user\" : username,\n",
    "\"password\" : password,\n",
    "\"driver\" : \"com.microsoft.sqlserver.jdbc.SQLServerDriver\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "pushdown_query = \"\"\"(SELECT * FROM SalesLT.DRM_Data) A\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read from AzureSQL\n",
    "df = spark.read.format(\"com.microsoft.sqlserver.jdbc.spark\").option(\"url\",\"jdbc:sqlserver://longth.database.windows.net:1433;database=LongTH\").\\\n",
    "    option(\"driver\",\"com.microsoft.sqlserver.jdbc.SQLServerDriver\").\\\n",
    "        option(\"dbtable\",pushdown_query).option(\"user\",username).\\\n",
    "            option(\"password\",password).load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------+----------+------------+\n",
      "|Total_Key|      Date|Service_Name|\n",
      "+---------+----------+------------+\n",
      "|       79|2019-08-02|         BHD|\n",
      "|     3513|2019-08-02|        Fim+|\n",
      "|     8816|2019-08-02|     PhimGoi|\n",
      "|       77|2019-08-03|         BHD|\n",
      "|     4039|2019-08-03|        Fim+|\n",
      "|     9294|2019-08-03|     PhimGoi|\n",
      "|       81|2019-08-04|         BHD|\n",
      "|     3963|2019-08-04|        Fim+|\n",
      "|     9460|2019-08-04|     PhimGoi|\n",
      "|       68|2019-08-05|         BHD|\n",
      "|     3194|2019-08-05|        Fim+|\n",
      "|     9541|2019-08-05|     PhimGoi|\n",
      "|       85|2019-08-06|         BHD|\n",
      "|     3541|2019-08-06|        Fim+|\n",
      "|     9793|2019-08-06|     PhimGoi|\n",
      "|       78|2019-08-07|         BHD|\n",
      "|     3441|2019-08-07|        Fim+|\n",
      "|    10958|2019-08-07|     PhimGoi|\n",
      "|       82|2019-08-08|         BHD|\n",
      "|     3566|2019-08-08|        Fim+|\n",
      "+---------+----------+------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
